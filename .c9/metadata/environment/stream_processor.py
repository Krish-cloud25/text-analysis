{"filter":false,"title":"stream_processor.py","tooltip":"/stream_processor.py","undoManager":{"mark":39,"position":39,"stack":[[{"start":{"row":0,"column":0},"end":{"row":28,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","ssc = StreamingContext(sc, 5)  # 5-second batch interval","","# Connect to the socket","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Split words and count in 10-second window, sliding every 5 seconds","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b, None, 10, 5",")","","# Print top 5 words every 5 seconds","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    print(\"Top 5 words in last 10s:\")","    for word, count in top5:","        print(f\"{word}: {count}\")","","windowed_counts.foreachRDD(print_top_rdd)","","ssc.start()","ssc.awaitTermination()",""],"id":95}],[{"start":{"row":4,"column":0},"end":{"row":4,"column":1},"action":"insert","lines":[" "],"id":96}],[{"start":{"row":4,"column":0},"end":{"row":4,"column":1},"action":"remove","lines":[" "],"id":97}],[{"start":{"row":28,"column":0},"end":{"row":29,"column":0},"action":"insert","lines":["",""],"id":98},{"start":{"row":29,"column":0},"end":{"row":30,"column":0},"action":"insert","lines":["",""]}],[{"start":{"row":30,"column":0},"end":{"row":39,"column":0},"action":"insert","lines":["sc = SparkContext(\"local[*]\", \"StreamWordCount\")","ssc = StreamingContext(sc, 5)","","lines = ssc.socketTextStream(\"localhost\", 9999)","","lines.pprint()  # <<< ADD THIS","","words = lines.flatMap(lambda line: line.split())","...",""],"id":99}],[{"start":{"row":7,"column":47},"end":{"row":8,"column":0},"action":"insert","lines":["",""],"id":100},{"start":{"row":8,"column":0},"end":{"row":9,"column":0},"action":"insert","lines":["",""]},{"start":{"row":9,"column":0},"end":{"row":10,"column":0},"action":"insert","lines":["",""]}],[{"start":{"row":10,"column":0},"end":{"row":10,"column":14},"action":"insert","lines":["lines.pprint()"],"id":101}],[{"start":{"row":29,"column":0},"end":{"row":42,"column":0},"action":"remove","lines":["ssc.start()","ssc.awaitTermination()","","","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","ssc = StreamingContext(sc, 5)","","lines = ssc.socketTextStream(\"localhost\", 9999)","","lines.pprint()  # <<< ADD THIS","","words = lines.flatMap(lambda line: line.split())","...",""],"id":102}],[{"start":{"row":9,"column":0},"end":{"row":10,"column":0},"action":"remove","lines":["",""],"id":103}],[{"start":{"row":0,"column":0},"end":{"row":28,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","ssc = StreamingContext(sc, 5)  # 5-second batch interval","","# Connect to the socket","lines = ssc.socketTextStream(\"localhost\", 9999)","","lines.pprint()","","# Split words and count in 10-second window, sliding every 5 seconds","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b, None, 10, 5",")","","# Print top 5 words every 5 seconds","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    print(\"Top 5 words in last 10s:\")","    for word, count in top5:","        print(f\"{word}: {count}\")","","windowed_counts.foreachRDD(print_top_rdd)","",""],"id":104}],[{"start":{"row":0,"column":0},"end":{"row":29,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"INFO\")  # ✅ Show more info","ssc = StreamingContext(sc, 5)","","# Connect to the socket","lines = ssc.socketTextStream(\"localhost\", 9999)","","# DEBUG: Show incoming lines","lines.pprint()","","# Count words in sliding window","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    print(\"Top 5 words in last 10s:\")","    for word, count in top5:","        print(f\"{word}: {count}\")","","windowed_counts.foreachRDD(print_top_rdd)","","ssc.start()","ssc.awaitTermination()",""],"id":105}],[{"start":{"row":0,"column":0},"end":{"row":29,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"INFO\")  # ✅ Show more info","ssc = StreamingContext(sc, 5)","","# Connect to the socket","lines = ssc.socketTextStream(\"localhost\", 9999)","","# DEBUG: Show incoming lines","lines.pprint()","","# Count words in sliding window","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    print(\"Top 5 words in last 10s:\")","    for word, count in top5:","        print(f\"{word}: {count}\")","","windowed_counts.foreachRDD(print_top_rdd)","","ssc.start()","ssc.awaitTermination()",""],"id":106}],[{"start":{"row":0,"column":0},"end":{"row":27,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"INFO\")","ssc = StreamingContext(sc, 5)","","lines = ssc.socketTextStream(\"localhost\", 9999)","lines.pprint()  # ✅ Add this line to see incoming text","","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Sliding window word count: window = 10s, slide = 5s","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    print(\"Top 5 words in last 10s:\")","    for word, count in top5:","        print(f\"{word}: {count}\")","","windowed_counts.foreachRDD(print_top_rdd)","","ssc.start()","ssc.awaitTermination()",""],"id":107}],[{"start":{"row":0,"column":0},"end":{"row":27,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"INFO\")","ssc = StreamingContext(sc, 5)","","lines = ssc.socketTextStream(\"localhost\", 9999)","lines.pprint()  # ✅ Add this line to see incoming text","","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Sliding window word count: window = 10s, slide = 5s","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    print(\"Top 5 words in last 10s:\")","    for word, count in top5:","        print(f\"{word}: {count}\")","","windowed_counts.foreachRDD(print_top_rdd)","","ssc.start()","ssc.awaitTermination()",""],"id":108}],[{"start":{"row":0,"column":0},"end":{"row":38,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize Spark context and set minimal logging","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Reduce Spark log noise","","# Create StreamingContext with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Connect to the stream server (localhost:9999)","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Print each line (for debugging)","lines.pprint()","","# Tokenize and clean words","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Perform sliding window word count","# Window duration: 10s, Slide interval: 5s","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","# Print top 5 words in each 10-second window","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    print(\"\\n==== Top 5 words in last 10s ====\")","    for word, count in top5:","        print(f\"{word}: {count}\")","","# Call the function for each RDD in the windowed DStream","windowed_counts.foreachRDD(print_top_rdd)","","# Start the streaming job","ssc.start()","ssc.awaitTermination()",""],"id":109}],[{"start":{"row":0,"column":0},"end":{"row":38,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize Spark context and set minimal logging","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Reduce Spark log noise","","# Create StreamingContext with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Connect to the stream server (localhost:9999)","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Print each line (for debugging)","lines.pprint()","","# Tokenize and clean words","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Perform sliding window word count","# Window duration: 10s, Slide interval: 5s","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","# Print top 5 words in each 10-second window","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    print(\"\\n==== Top 5 words in last 10s ====\")","    for word, count in top5:","        print(f\"{word}: {count}\")","","# Call the function for each RDD in the windowed DStream","windowed_counts.foreachRDD(print_top_rdd)","","# Start the streaming job","ssc.start()","ssc.awaitTermination()",""],"id":110}],[{"start":{"row":0,"column":0},"end":{"row":38,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize Spark context and reduce unnecessary logging","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Only show errors","","# Create a StreamingContext with a 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Connect to the socket stream from stream_server.py","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Debug: Print each incoming line to console","lines.pprint()","","# Tokenize lines into words and convert to lowercase","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Apply sliding window: 10-second window, sliding every 5 seconds","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","# Function to print top 5 words in each window","def print_top_rdd(rdd):","    if not rdd.isEmpty():","        sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","        top5 = sorted_rdd.take(5)","        print(\"\\n==== Top 5 words in last 10s ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Attach the print function to each RDD in the DStream","windowed_counts.foreachRDD(print_top_rdd)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":111}],[{"start":{"row":0,"column":0},"end":{"row":38,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize Spark context and reduce unnecessary logging","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Only show errors","","# Create a StreamingContext with a 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Connect to the socket stream from stream_server.py","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Debug: Print each incoming line to console","lines.pprint()","","# Tokenize lines into words and convert to lowercase","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Apply sliding window: 10-second window, sliding every 5 seconds","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","# Function to print top 5 words in each window","def print_top_rdd(rdd):","    if not rdd.isEmpty():","        sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","        top5 = sorted_rdd.take(5)","        print(\"\\n==== Top 5 words in last 10s ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Attach the print function to each RDD in the DStream","windowed_counts.foreachRDD(print_top_rdd)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":112}],[{"start":{"row":0,"column":0},"end":{"row":45,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize SparkContext and reduce log noise","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")","","# Set up StreamingContext with a 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Read text from the stream server (localhost:9999)","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Show incoming text for debugging","lines.pprint()","","# Tokenize the incoming lines into words","words = lines.flatMap(lambda line: line.split())","","# Convert words to lowercase and pair with 1","pairs = words.map(lambda word: (word.lower(), 1))","","# Sliding window of 10 seconds, sliding every 5 seconds","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,     # reduceFunc","    None,                   # inverseFunc (None = full recomputation)","    10,                     # window duration in seconds","    5                       # slide interval in seconds",")","","# Print top 5 words in each window","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10s ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Apply the function to each RDD in the stream","windowed_counts.foreachRDD(print_top_rdd)","","# Start streaming and wait for termination","ssc.start()","ssc.awaitTermination()",""],"id":113}],[{"start":{"row":0,"column":0},"end":{"row":45,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize SparkContext and reduce log noise","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")","","# Set up StreamingContext with a 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Read text from the stream server (localhost:9999)","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Show incoming text for debugging","lines.pprint()","","# Tokenize the incoming lines into words","words = lines.flatMap(lambda line: line.split())","","# Convert words to lowercase and pair with 1","pairs = words.map(lambda word: (word.lower(), 1))","","# Sliding window of 10 seconds, sliding every 5 seconds","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,     # reduceFunc","    None,                   # inverseFunc (None = full recomputation)","    10,                     # window duration in seconds","    5                       # slide interval in seconds",")","","# Print top 5 words in each window","def print_top_rdd(rdd):","    sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","    top5 = sorted_rdd.take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10s ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Apply the function to each RDD in the stream","windowed_counts.foreachRDD(print_top_rdd)","","# Start streaming and wait for termination","ssc.start()","ssc.awaitTermination()",""],"id":114}],[{"start":{"row":0,"column":0},"end":{"row":45,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize SparkContext","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Minimize logs","","# Streaming context with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Connect to socket stream","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Print incoming lines (for debug)","lines.pprint()","","# Tokenize words","words = lines.flatMap(lambda line: line.split())","","# Map each word to (word, 1)","pairs = words.map(lambda word: (word.lower(), 1))","","# Windowed word count: 10-second window, sliding every 5 seconds","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    lambda a, b: a - b,  # Optional inverse function for efficiency","    windowDuration=10,","    slideDuration=5",")","","# Print top 5 words for each window","def print_top_words(rdd):","    if not rdd.isEmpty():","        sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","        top5 = sorted_rdd.take(5)","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Output results","windowed_counts.foreachRDD(print_top_words)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":115}],[{"start":{"row":0,"column":0},"end":{"row":45,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize SparkContext","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Minimize logs","","# Streaming context with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Connect to socket stream","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Print incoming lines (for debug)","lines.pprint()","","# Tokenize words","words = lines.flatMap(lambda line: line.split())","","# Map each word to (word, 1)","pairs = words.map(lambda word: (word.lower(), 1))","","# Windowed word count: 10-second window, sliding every 5 seconds","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    lambda a, b: a - b,  # Optional inverse function for efficiency","    windowDuration=10,","    slideDuration=5",")","","# Print top 5 words for each window","def print_top_words(rdd):","    if not rdd.isEmpty():","        sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)","        top5 = sorted_rdd.take(5)","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Output results","windowed_counts.foreachRDD(print_top_words)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":116}],[{"start":{"row":0,"column":0},"end":{"row":45,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize SparkContext","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Reduce Spark log noise","","# Create StreamingContext with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# ✅ Set checkpoint directory (required for stateful ops like reduceByKeyAndWindow)","ssc.checkpoint(\"checkpoint_dir\")","","# Connect to the stream server","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Show each incoming line","lines.pprint()","","# Process words","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Perform sliding window word count","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    lambda a, b: a - b,","    windowDuration=10,","    slideDuration=5",")","","# Print top 5 words from each window","def print_top_words(rdd):","    if not rdd.isEmpty():","        top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Apply to each RDD in windowed DStream","windowed_counts.foreachRDD(print_top_words)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":117}],[{"start":{"row":45,"column":0},"end":{"row":45,"column":1},"action":"insert","lines":["c"],"id":118},{"start":{"row":45,"column":1},"end":{"row":45,"column":2},"action":"insert","lines":["l"]},{"start":{"row":45,"column":2},"end":{"row":45,"column":3},"action":"insert","lines":["e"]},{"start":{"row":45,"column":3},"end":{"row":45,"column":4},"action":"insert","lines":["a"]}],[{"start":{"row":45,"column":3},"end":{"row":45,"column":4},"action":"remove","lines":["a"],"id":119},{"start":{"row":45,"column":2},"end":{"row":45,"column":3},"action":"remove","lines":["e"]},{"start":{"row":45,"column":1},"end":{"row":45,"column":2},"action":"remove","lines":["l"]},{"start":{"row":45,"column":0},"end":{"row":45,"column":1},"action":"remove","lines":["c"]}],[{"start":{"row":44,"column":22},"end":{"row":45,"column":0},"action":"remove","lines":["",""],"id":120}],[{"start":{"row":0,"column":0},"end":{"row":44,"column":22},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize SparkContext","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Reduce Spark log noise","","# Create StreamingContext with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# ✅ Set checkpoint directory (required for stateful ops like reduceByKeyAndWindow)","ssc.checkpoint(\"checkpoint_dir\")","","# Connect to the stream server","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Show each incoming line","lines.pprint()","","# Process words","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Perform sliding window word count","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    lambda a, b: a - b,","    windowDuration=10,","    slideDuration=5",")","","# Print top 5 words from each window","def print_top_words(rdd):","    if not rdd.isEmpty():","        top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Apply to each RDD in windowed DStream","windowed_counts.foreachRDD(print_top_words)","","# Start streaming","ssc.start()","ssc.awaitTermination()"],"id":121}],[{"start":{"row":0,"column":0},"end":{"row":45,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize Spark context","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Suppress excessive logging","","# Create StreamingContext with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# 🔐 Set checkpoint directory (mandatory for window operations)","ssc.checkpoint(\"checkpoint_dir\")","","# Connect to the stream source","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Optional: Log number of lines received per batch (debug)","lines.foreachRDD(lambda rdd: print(f\">>> Received {rdd.count()} lines\"))","","# Print incoming lines (for debug)","lines.pprint()","","# Tokenize, clean, and count","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Sliding window word count: window = 10s, slide = 5s","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b, None, 10, 5",")","","# Show top 5 words in every 10-second window","def print_top_rdd(rdd):","    top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Register callback","windowed_counts.foreachRDD(print_top_rdd)","","# Start and wait for termination","ssc.start()","ssc.awaitTermination()",""],"id":122}],[{"start":{"row":0,"column":0},"end":{"row":45,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize Spark context","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")  # Suppress excessive logging","","# Create StreamingContext with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# 🔐 Set checkpoint directory (mandatory for window operations)","ssc.checkpoint(\"checkpoint_dir\")","","# Connect to the stream source","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Optional: Log number of lines received per batch (debug)","lines.foreachRDD(lambda rdd: print(f\">>> Received {rdd.count()} lines\"))","","# Print incoming lines (for debug)","lines.pprint()","","# Tokenize, clean, and count","words = lines.flatMap(lambda line: line.split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Sliding window word count: window = 10s, slide = 5s","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b, None, 10, 5",")","","# Show top 5 words in every 10-second window","def print_top_rdd(rdd):","    top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","# Register callback","windowed_counts.foreachRDD(print_top_rdd)","","# Start and wait for termination","ssc.start()","ssc.awaitTermination()",""],"id":123}],[{"start":{"row":0,"column":0},"end":{"row":45,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize Spark context","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")","","# Create StreamingContext with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Set checkpoint (required for reduceByKeyAndWindow)","ssc.checkpoint(\"checkpoint\")","","# Read from socket stream","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Debug: print each line received","lines.foreachRDD(lambda rdd: print(f\"Received {rdd.count()} lines\"))","","# Tokenize and normalize","words = lines.flatMap(lambda line: line.strip().split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Perform windowed word count: window=10s, slide=5s","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    None,  # no inverse function needed","    10, 5",")","","# Print top 5 words from the current window","def print_top_rdd(rdd):","    top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","    else:","        print(\"No data received in this window.\")","","windowed_counts.foreachRDD(print_top_rdd)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":124}],[{"start":{"row":0,"column":0},"end":{"row":45,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize Spark context","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")","","# Create StreamingContext with 5-second batch interval","ssc = StreamingContext(sc, 5)","","# Set checkpoint (required for reduceByKeyAndWindow)","ssc.checkpoint(\"checkpoint\")","","# Read from socket stream","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Debug: print each line received","lines.foreachRDD(lambda rdd: print(f\"Received {rdd.count()} lines\"))","","# Tokenize and normalize","words = lines.flatMap(lambda line: line.strip().split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Perform windowed word count: window=10s, slide=5s","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    None,  # no inverse function needed","    10, 5",")","","# Print top 5 words from the current window","def print_top_rdd(rdd):","    top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","    else:","        print(\"No data received in this window.\")","","windowed_counts.foreachRDD(print_top_rdd)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":125}],[{"start":{"row":0,"column":0},"end":{"row":41,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize context and streaming","sc = SparkContext(\"local[*]\", \"NetworkWordCount\")","sc.setLogLevel(\"ERROR\")","ssc = StreamingContext(sc, 5)","ssc.checkpoint(\"checkpoint\")  # Required for window operations","","# Connect to stream","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Split lines into words","words = lines.flatMap(lambda line: line.strip().split())","","# Map each word to (word, 1)","pairs = words.map(lambda word: (word.lower(), 1))","","# Windowed word count over 10s window every 5s","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    None,","    windowDuration=10,","    slideDuration=5",")","","# Print top 5 words if there are any","def print_top_words(rdd):","    top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","    else:","        print(\"No data in this window.\")","","windowed_counts.foreachRDD(print_top_words)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":126}],[{"start":{"row":0,"column":0},"end":{"row":41,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize context and streaming","sc = SparkContext(\"local[*]\", \"NetworkWordCount\")","sc.setLogLevel(\"ERROR\")","ssc = StreamingContext(sc, 5)","ssc.checkpoint(\"checkpoint\")  # Required for window operations","","# Connect to stream","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Split lines into words","words = lines.flatMap(lambda line: line.strip().split())","","# Map each word to (word, 1)","pairs = words.map(lambda word: (word.lower(), 1))","","# Windowed word count over 10s window every 5s","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    None,","    windowDuration=10,","    slideDuration=5",")","","# Print top 5 words if there are any","def print_top_words(rdd):","    top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","    else:","        print(\"No data in this window.\")","","windowed_counts.foreachRDD(print_top_words)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":127}],[{"start":{"row":0,"column":0},"end":{"row":39,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# 1. Create SparkContext & StreamingContext","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")","ssc = StreamingContext(sc, 5)  # 5-second batch interval","ssc.checkpoint(\"checkpoint\")  # Required for windowed operations","","# 2. Read from socket","lines = ssc.socketTextStream(\"localhost\", 9999)","","# 3. Transform: split → map → reduce","words = lines.flatMap(lambda line: line.strip().split())","pairs = words.map(lambda word: (word.lower(), 1))","","# 4. Windowed word count (10s window, 5s slide)","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    None,","    windowDuration=10,","    slideDuration=5",")","","# 5. Print top 5 words","def print_top_words(rdd):","    top = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top:","            print(f\"{word}: {count}\")","    else:","        print(\"No data received.\")","","windowed_counts.foreachRDD(print_top_words)","","# 6. Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":128}],[{"start":{"row":0,"column":0},"end":{"row":39,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# 1. Create SparkContext & StreamingContext","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")","ssc = StreamingContext(sc, 5)  # 5-second batch interval","ssc.checkpoint(\"checkpoint\")  # Required for windowed operations","","# 2. Read from socket","lines = ssc.socketTextStream(\"localhost\", 9999)","","# 3. Transform: split → map → reduce","words = lines.flatMap(lambda line: line.strip().split())","pairs = words.map(lambda word: (word.lower(), 1))","","# 4. Windowed word count (10s window, 5s slide)","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    None,","    windowDuration=10,","    slideDuration=5",")","","# 5. Print top 5 words","def print_top_words(rdd):","    top = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top:","            print(f\"{word}: {count}\")","    else:","        print(\"No data received.\")","","windowed_counts.foreachRDD(print_top_words)","","# 6. Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":129}],[{"start":{"row":0,"column":0},"end":{"row":32,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Step 1: Create Spark Context and Streaming Context","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")","ssc = StreamingContext(sc, 5)  # 5-second batch interval","ssc.checkpoint(\"checkpoint\")  # Checkpoint required for windowed operations","","# Step 2: Connect to socket stream","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Step 3: Process the data","words = lines.flatMap(lambda line: line.strip().split())","pairs = words.map(lambda word: (word.lower(), 1))","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","# Step 4: Print top 5 words","def print_top_rdd(rdd):","    top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","    else:","        print(\"No data in this window.\")","","windowed_counts.foreachRDD(print_top_rdd)","","# Step 5: Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":130}],[{"start":{"row":0,"column":0},"end":{"row":32,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Step 1: Create Spark Context and Streaming Context","sc = SparkContext(\"local[*]\", \"StreamWordCount\")","sc.setLogLevel(\"ERROR\")","ssc = StreamingContext(sc, 5)  # 5-second batch interval","ssc.checkpoint(\"checkpoint\")  # Checkpoint required for windowed operations","","# Step 2: Connect to socket stream","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Step 3: Process the data","words = lines.flatMap(lambda line: line.strip().split())","pairs = words.map(lambda word: (word.lower(), 1))","windowed_counts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, None, 10, 5)","","# Step 4: Print top 5 words","def print_top_rdd(rdd):","    top5 = rdd.sortBy(lambda x: x[1], ascending=False).take(5)","    if top5:","        print(\"\\n==== Top 5 words in last 10 seconds ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","    else:","        print(\"No data in this window.\")","","windowed_counts.foreachRDD(print_top_rdd)","","# Step 5: Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":131}],[{"start":{"row":0,"column":0},"end":{"row":41,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize SparkContext and reduce logs","sc = SparkContext(\"local[2]\", \"FastStreamWordCount\")","sc.setLogLevel(\"ERROR\")","","# Create StreamingContext with 2-second batch (shorter for fast input)","ssc = StreamingContext(sc, 2)","","# Required checkpoint directory for window ops","ssc.checkpoint(\"checkpoint_dir\")","","# Connect to stream server","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Split into words","words = lines.flatMap(lambda line: line.strip().split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Sliding window word count: 6 sec window, slide every 2 sec","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    lambda a, b: a - b,","    6,","    2",")","","# Print top 5 words","def print_top_rdd(rdd):","    results = rdd.takeOrdered(5, key=lambda x: -x[1])","    if results:","        print(\"\\n==== Top 5 words in last 6 seconds ====\")","        for word, count in results:","            print(f\"{word}: {count}\")","","windowed_counts.foreachRDD(print_top_rdd)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":132}],[{"start":{"row":0,"column":0},"end":{"row":41,"column":0},"action":"remove","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Initialize SparkContext and reduce logs","sc = SparkContext(\"local[2]\", \"FastStreamWordCount\")","sc.setLogLevel(\"ERROR\")","","# Create StreamingContext with 2-second batch (shorter for fast input)","ssc = StreamingContext(sc, 2)","","# Required checkpoint directory for window ops","ssc.checkpoint(\"checkpoint_dir\")","","# Connect to stream server","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Split into words","words = lines.flatMap(lambda line: line.strip().split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Sliding window word count: 6 sec window, slide every 2 sec","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda a, b: a + b,","    lambda a, b: a - b,","    6,","    2",")","","# Print top 5 words","def print_top_rdd(rdd):","    results = rdd.takeOrdered(5, key=lambda x: -x[1])","    if results:","        print(\"\\n==== Top 5 words in last 6 seconds ====\")","        for word, count in results:","            print(f\"{word}: {count}\")","","windowed_counts.foreachRDD(print_top_rdd)","","# Start streaming","ssc.start()","ssc.awaitTermination()",""],"id":133}],[{"start":{"row":0,"column":0},"end":{"row":38,"column":0},"action":"insert","lines":["from pyspark import SparkContext","from pyspark.streaming import StreamingContext","","# Create SparkContext and StreamingContext","sc = SparkContext(\"local[2]\", \"StreamingApp\")","sc.setLogLevel(\"ERROR\")","ssc = StreamingContext(sc, 2)  # 2s batch","","# Checkpoint is required for reduceByKeyAndWindow","ssc.checkpoint(\"checkpoint\")","","# Create DStream from socket","lines = ssc.socketTextStream(\"localhost\", 9999)","","# Process the stream","words = lines.flatMap(lambda line: line.strip().split())","pairs = words.map(lambda word: (word.lower(), 1))","","# Reduce over window: 10s window, 2s slide","windowed_counts = pairs.reduceByKeyAndWindow(","    lambda x, y: x + y,","    lambda x, y: x - y,","    10,","    2",")","","def print_top_rdd(rdd):","    sorted_items = rdd.sortBy(lambda x: -x[1])","    top5 = sorted_items.take(5)","    if top5:","        print(\"\\n==== Top 5 Words ====\")","        for word, count in top5:","            print(f\"{word}: {count}\")","","windowed_counts.foreachRDD(print_top_rdd)","","ssc.start()","ssc.awaitTermination()",""],"id":134}]]},"ace":{"folds":[],"scrolltop":0,"scrollleft":0,"selection":{"start":{"row":38,"column":0},"end":{"row":38,"column":0},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1751973455231,"hash":"e9baa62413713ed445b57834aa254bbbb3285b3e"}